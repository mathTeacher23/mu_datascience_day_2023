{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9b7a40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from itertools import combinations \n",
    "from functools import reduce\n",
    "\n",
    "def conditions(string):\n",
    "    \n",
    "    '''Create conditional logic for identifying duplicates based on tiered levels of confidence'''\n",
    "    \n",
    "    if 'Full Name' in string and 'Full Address' in string and 'Email' in string and 'Phone' in string:\n",
    "        \n",
    "        return 'Exact Match'\n",
    "    \n",
    "    elif 'Full Address' in string and 'Email' in string and 'Phone' in string:\n",
    "        \n",
    "        return 'High Confidence'\n",
    "    \n",
    "    elif 'Full Address' in string and 'Email' in string:\n",
    "        return 'Medium Confidence'\n",
    "    else:\n",
    "        return 'Low Confidence'\n",
    "    \n",
    "def create_match_confidence_reference():\n",
    "    # initialize lists\n",
    "    list_ = [\"Full Name\", \"Full Address\", \"Email\", \"Phone\"]\n",
    "    \n",
    "    unique_combinations = []\n",
    "    for n in range(1, 5, 1):\n",
    "        combos = itertools.combinations(list_, n)\n",
    "        combos = [sorted(x) for x in combos]\n",
    "        strings = [', '.join(combo) for combo in sorted(combos)]\n",
    "        unique_combinations.append(strings)\n",
    "\n",
    "    unique_combinations = sorted(list(itertools.chain(*unique_combinations)))\n",
    "\n",
    "    confidence_df = pd.DataFrame({'Combination': unique_combinations})\n",
    "    confidence_df['Confidence'] = confidence_df['Combination'].apply(conditions)\n",
    "\n",
    "    custom_order = ['Exact Match', 'High Confidence', 'Medium Confidence', 'Low Confidence']\n",
    "    confidence_df['Confidence'] = pd.Categorical(confidence_df['Confidence'], categories=custom_order, ordered=True)\n",
    "    confidence_df = confidence_df.sort_values(by='Confidence').reset_index().drop(columns = 'index')\n",
    "    confidence_df.loc[len(confidence_df.index)] = ['NO MATCH', 'NO MATCH'] \n",
    "    return(confidence_df)\n",
    "\n",
    "def generate_fake_pii_df(sample_size):\n",
    "    '''\n",
    "    Simulate data where names, addresses,phone numbers and emails are present across multiple IDs.\n",
    "    This simulates where duplicate cases are present in the data and how potential fraud could be identified.\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=['Name', 'Address', 'Phone', 'Email'])\n",
    "    for i in range(sample_size):\n",
    "        name = random.choice(['Andrew Casanova', 'Steve Tedford', 'Lindsey Tagg', 'John Smith', 'Jada Pinkett', 'Melissa Smith'])\n",
    "        address = random.choice(['333 Miller Road', '444 Daisy Court', '111 Apple Orchard Ave', '222 Adams Boulevard'])\n",
    "        phone = random.choice(['845-457-5494', '703-398-6403', '713-456-1234', '808-841-2406'])\n",
    "        email = random.choice(['rosepetal13@gmail.com', 'america21@hotmail.com', 'wolfman21@hotmail.com', 'livenation@gmail.com'])\n",
    "        df.loc[i] = [name, address, phone, email]\n",
    "\n",
    "    df['Previous_ID'] = range(0, len(df))\n",
    "    df['Previous_ID'] = df['Previous_ID'].astype(str)\n",
    "    return(df)\n",
    "\n",
    "def get_matches(data, ids, column_to_match_on):\n",
    "    '''\n",
    "    Iterate over each id and get the value for the PII column and pull a list of IDs that share the same PII.\n",
    "    If there is no value to pull then simply append its own id to the list. \n",
    "    Create a dataframe of the ID, the PII value, and the list of IDs that shared the same value.\n",
    "    '''\n",
    "    all_relations = []\n",
    "    for app_id in ids:\n",
    "        value = data[data['Previous_ID'] == app_id][column_to_match_on].values[0]\n",
    "        if value == '':\n",
    "            related_ids = [app_id]\n",
    "        else:\n",
    "            related_ids = data[data[column_to_match_on] == value]['Previous_ID'].values.tolist()\n",
    "        all_relations.append([app_id, value, related_ids])\n",
    "        \n",
    "    match_df = pd.DataFrame(all_relations)\n",
    "    match_df.columns = ['Previous_ID', column_to_match_on, '{}_Related_Ids'.format(column_to_match_on)]\n",
    "    return(match_df)\n",
    "\n",
    "\n",
    "def merge_dataframe_list(df_list, merge_on):\n",
    "    '''Merge list of matching dataframes for each PII into a single dataframe'''\n",
    "    #if multiple dataframes want to combine column-wise use reduce from functools\n",
    "    merged_df = reduce(lambda x, y: pd.merge(x,y, on = merge_on), df_list)\n",
    "    \n",
    "    all_related_ids = []\n",
    "    for row in merged_df.itertuples():\n",
    "        all_ids = row.Name_Related_Ids + row.Address_Related_Ids + row.Phone_Related_Ids + row.Email_Related_Ids \n",
    "        all_ids = list(set(all_ids))\n",
    "        #all_ids = [ele for ele in all_ids if ele != row.Previous_ID]\n",
    "        all_related_ids.append(sorted(all_ids))\n",
    "        \n",
    "    merged_df['All_Related_Ids'] = all_related_ids\n",
    "    \n",
    "    return(merged_df)\n",
    "\n",
    "def get_match_strings(related_id_cols, match_df):\n",
    "    '''\n",
    "    Iterate over each list of related ids for every row and check if id is in any of the PII_Related_ID cols.\n",
    "    If it is then append the name of the PII_Related_ID column to a string and build out the string specifying all \n",
    "    PII matched for each id in the list.\n",
    "    '''\n",
    "    confidence_df = create_match_confidence_reference()\n",
    "    \n",
    "    #Create a label of pii matched on and calculate similarity scores.\n",
    "    all_edge_labels = []\n",
    "    all_related_ids = []\n",
    "    all_confidences = []\n",
    "    for row in match_df.itertuples():\n",
    "        ids = row.All_Related_Ids\n",
    "        edge_label = []\n",
    "        new_ids = []\n",
    "        confidences = []\n",
    "        for id in ids:\n",
    "            id_matched_on = []\n",
    "            if id in row.Name_Related_Ids:\n",
    "                id_matched_on.append('Full Name')\n",
    "            if id in row.Address_Related_Ids:\n",
    "                id_matched_on.append('Full Address')\n",
    "            if id in row.Email_Related_Ids:\n",
    "                id_matched_on.append('Email')\n",
    "            if id in row.Phone_Related_Ids:\n",
    "                id_matched_on.append('Phone')\n",
    "            else:\n",
    "                #id_matched_on.append('NO MATCH')\n",
    "                pass\n",
    "            \n",
    "            sorted_id_matched_on = sorted(list(set(id_matched_on)))\n",
    "            label = ', '.join(sorted_id_matched_on)\n",
    "            \n",
    "            #Remove low confidence matches\n",
    "            match_type = confidence_df[confidence_df['Combination'] == label]['Confidence'].values[0]\n",
    "            \n",
    "            if match_type != 'Low Confidence':\n",
    "                edge_label.append(label)\n",
    "                new_ids.append(id)\n",
    "                confidences.append(match_type)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        all_related_ids.append(new_ids)\n",
    "        all_edge_labels.append(edge_label)\n",
    "        all_confidences.append(confidences)\n",
    "    match_df['Edge_Label'] = all_edge_labels\n",
    "    match_df['New_All_Related_Ids'] = all_related_ids\n",
    "    match_df['Confidence_Lists'] = all_confidences\n",
    "    return(match_df)\n",
    "\n",
    "def assign_group_id(all_related_ids):\n",
    "    '''\n",
    "    Identify a unique list of groups and assign the unique group an ID. Here there will be overlap across groupings.\n",
    "    This is the case where an id matched on email against one case but matched all other PII with other cases. \n",
    "    This would be helpful in trying to determine if fraudulent behavior is occurring across other applications.\n",
    "    '''\n",
    "    unique_lists = []\n",
    "    id = 0\n",
    "    for lst in all_related_ids:\n",
    "        if lst not in unique_lists:\n",
    "            unique_lists.append([id, sorted(lst)])\n",
    "            id += 1\n",
    "        else:\n",
    "            pass\n",
    "    return(unique_lists)\n",
    "\n",
    "def similarity_score(label):\n",
    "    '''Get the proportion of PII matched out of the total number of all PII that could be matched'''\n",
    "    score = 0\n",
    "    if 'Full Name' in label:\n",
    "        score += 1\n",
    "    if 'Full Address' in label:\n",
    "        score += 1\n",
    "    if 'Email' in label:\n",
    "        score +=1\n",
    "    if 'Phone' in label:\n",
    "        score += 1\n",
    "    return(score/4)\n",
    "\n",
    "def weighted_similarity_score(label):\n",
    "    '''Weight PII differently and get the proportion of PII matched out of the total weight of all PII'''\n",
    "    score = 0\n",
    "    if 'Full Name' in label:\n",
    "        score += 2\n",
    "    if 'Full Address' in label:\n",
    "        score += 3\n",
    "    if 'Email' in label:\n",
    "        score += 2\n",
    "    if 'Phone' in label:\n",
    "        score += 1\n",
    "    return(score/8)\n",
    "\n",
    "\n",
    "create_match_confidence_reference()\n",
    "    \n",
    "def main():\n",
    "    #Simulate fake pii data and begin iterating over each row to get list of related ids\n",
    "    df = generate_fake_pii_df(50)\n",
    "    \n",
    "    #Determine a list of ids for which to begin matching against (in this case all the ids in the data)\n",
    "    ids = df['Previous_ID'].astype(str).tolist()\n",
    "\n",
    "    columns_to_match = ['Name', 'Email', 'Phone', 'Address']\n",
    "\n",
    "    dataframes = []\n",
    "    for col in columns_to_match:\n",
    "        match_df = get_matches(df, ids, col)\n",
    "        dataframes.append(match_df)\n",
    "\n",
    "    merged_df = merge_dataframe_list(dataframes, merge_on = 'Previous_ID')\n",
    "    \n",
    "    #Create a column that specifies what each related id matched on\n",
    "    related_cols = ['Name_Related_Ids', 'Email_Related_Ids', 'Phone_Related_Ids', 'Address_Related_Ids']\n",
    "    final_df = get_match_strings(related_cols, merged_df)\n",
    "    \n",
    "    #Get a list of unique groups and assign the group an id (may have overlap between groups depending on pii matched)\n",
    "    unique_lists = assign_group_id(final_df['New_All_Related_Ids'])\n",
    "    \n",
    "    group_df = pd.DataFrame({'Group_ID' : [i[0] for i in unique_lists],\n",
    "                             'All_Related_Ids_String': [i[1] for i in unique_lists]})\n",
    "    \n",
    "    #Join the related ids together as a string so we can map the group id back to the related ids column\n",
    "    group_df['All_Related_Ids_String'] = group_df['All_Related_Ids_String'].apply(lambda x: ', '.join(x))\n",
    "    final_df['All_Related_Ids_String'] = final_df['New_All_Related_Ids'].apply(lambda x: ', '.join(x))\n",
    "    merged_df = pd.merge(final_df, group_df, on = 'All_Related_Ids_String', how = 'left')\n",
    "    merged_df = merged_df.drop(columns = 'All_Related_Ids_String')\n",
    "    \n",
    "    #Expand the 1:many dataframe to the 1:1 dataframe so that each relationship gets its own row\n",
    "    exploded_df = merged_df.explode(['New_All_Related_Ids', 'Edge_Label', 'Confidence_Lists'])\n",
    "    \n",
    "#     #Calculate similarity scores to determine how similar ids are to each other\n",
    "#     exploded_df['Similarity_Score'] = exploded_df['Edge_Label'].apply(lambda x: similarity_score(x))\n",
    "#     exploded_df['Weighted_Similarity_Score'] = exploded_df['Edge_Label'].apply(lambda x: weighted_similarity_score(x))\n",
    "    \n",
    "#     #Using a particular set of logical conditions, label the ids as a Exact Match, High, Medium, or Low Confidence duplicate \n",
    "#     func = np.vectorize(conditions)\n",
    "#     exploded_df['Is_Duplicate'] = func(exploded_df['Edge_Label'])\n",
    "    \n",
    "    return(merged_df, exploded_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "cd89d5c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df, exploded_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c2f6b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(df):\n",
    "    nodes = df['Previous_ID'].unique().tolist()\n",
    "    nodes = ['ID: ' + node for node in nodes ]\n",
    "    return(nodes)\n",
    "\n",
    "def create_edges(df):\n",
    "    edges_df = df[['Previous_ID', 'New_All_Related_Ids']]\n",
    "    edges_df = edges_df[edges_df['Previous_ID'] != edges_df['New_All_Related_Ids']]\n",
    "    edges = [('ID: ' + row['Previous_ID'], 'ID: ' + row['New_All_Related_Ids']) for index, row in edges_df.iterrows()]\n",
    "    return(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "94fb1601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "edges.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"edges.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd5bb4d5d30>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True)\n",
    "\n",
    "net.add_nodes(create_nodes(exploded_df))\n",
    "net.add_edges(create_edges(exploded_df))\n",
    "net.show('edges.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8ce63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
