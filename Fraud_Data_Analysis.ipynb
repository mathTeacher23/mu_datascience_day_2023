{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "058a792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import faker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from itertools import combinations \n",
    "from functools import reduce\n",
    "\n",
    "def conditions(string):\n",
    "    \n",
    "    '''Create conditional logic for identifying duplicates based on tiered levels of confidence'''\n",
    "    \n",
    "    if 'Full Name' in string and 'Full Address' in string and 'Email' in string and 'Phone' in string:\n",
    "        \n",
    "        return 'Exact Match'\n",
    "    \n",
    "    elif 'Full Address' in string and 'Email' in string and 'Phone' in string:\n",
    "        \n",
    "        return 'High Confidence'\n",
    "    \n",
    "    elif 'Full Address' in string and 'Email' in string:\n",
    "        return 'Medium Confidence'\n",
    "    else:\n",
    "        return 'Low Confidence'\n",
    "    \n",
    "def create_match_confidence_reference():\n",
    "    # initialize lists\n",
    "    list_ = [\"Full Name\", \"Full Address\", \"Email\", \"Phone\"]\n",
    "    \n",
    "    unique_combinations = []\n",
    "    for n in range(1, 5, 1):\n",
    "        combos = itertools.combinations(list_, n)\n",
    "        combos = [sorted(x) for x in combos]\n",
    "        strings = [', '.join(combo) for combo in sorted(combos)]\n",
    "        unique_combinations.append(strings)\n",
    "\n",
    "    unique_combinations = sorted(list(itertools.chain(*unique_combinations)))\n",
    "\n",
    "    confidence_df = pd.DataFrame({'Combination': unique_combinations})\n",
    "    confidence_df['Confidence'] = confidence_df['Combination'].apply(conditions)\n",
    "\n",
    "    custom_order = ['Exact Match', 'High Confidence', 'Medium Confidence', 'Low Confidence']\n",
    "    confidence_df['Confidence'] = pd.Categorical(confidence_df['Confidence'], categories=custom_order, ordered=True)\n",
    "    confidence_df = confidence_df.sort_values(by='Confidence').reset_index().drop(columns = 'index')\n",
    "    confidence_df.loc[len(confidence_df.index)] = ['NO MATCH', 'NO MATCH'] \n",
    "    return(confidence_df)\n",
    "\n",
    "def generate_fake_pii_df(sample_size):\n",
    "    '''\n",
    "    Simulate data where names, addresses,phone numbers and emails are present across multiple IDs.\n",
    "    This simulates where duplicate cases are present in the data and how potential fraud could be identified.\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=['Name', 'Address', 'Phone', 'Email'])\n",
    "    for i in range(sample_size):\n",
    "        name = random.choice(['Andrew Casanova', 'Steve Tedford', 'Lindsey Tagg', 'John Smith', 'Jada Pinkett', 'Melissa Smith'])\n",
    "        address = random.choice(['333 Miller Road', '444 Daisy Court', '111 Apple Orchard Ave', '222 Adams Boulevard'])\n",
    "        phone = random.choice(['845-457-5494', '703-398-6403', '713-456-1234', '808-841-2406'])\n",
    "        email = random.choice(['rosepetal13@gmail.com', 'america21@hotmail.com', 'wolfman21@hotmail.com', 'livenation@gmail.com'])\n",
    "        df.loc[i] = [name, address, phone, email]\n",
    "\n",
    "    df['Previous_ID'] = range(0, len(df))\n",
    "    df['Previous_ID'] = df['Previous_ID'].astype(str)\n",
    "    return(df)\n",
    "\n",
    "def get_matches(data, ids, column_to_match_on):\n",
    "    '''\n",
    "    Iterate over each id and get the value for the PII column and pull a list of IDs that share the same PII.\n",
    "    If there is no value to pull then simply append its own id to the list. \n",
    "    Create a dataframe of the ID, the PII value, and the list of IDs that shared the same value.\n",
    "    '''\n",
    "    all_relations = []\n",
    "    for app_id in ids:\n",
    "        value = data[data['Previous_ID'] == app_id][column_to_match_on].values[0]\n",
    "        if value == '':\n",
    "            related_ids = [app_id]\n",
    "        else:\n",
    "            related_ids = data[data[column_to_match_on] == value]['Previous_ID'].values.tolist()\n",
    "        all_relations.append([app_id, value, related_ids])\n",
    "        \n",
    "    match_df = pd.DataFrame(all_relations)\n",
    "    match_df.columns = ['Previous_ID', column_to_match_on, '{}_Related_Ids'.format(column_to_match_on)]\n",
    "    return(match_df)\n",
    "\n",
    "\n",
    "def merge_dataframe_list(df_list, merge_on):\n",
    "    '''Merge list of matching dataframes for each PII into a single dataframe'''\n",
    "    #if multiple dataframes want to combine column-wise use reduce from functools\n",
    "    merged_df = reduce(lambda x, y: pd.merge(x,y, on = merge_on), df_list)\n",
    "    \n",
    "    all_related_ids = []\n",
    "    for row in merged_df.itertuples():\n",
    "        all_ids = row.Name_Related_Ids + row.Address_Related_Ids + row.Phone_Related_Ids + row.Email_Related_Ids \n",
    "        all_ids = list(set(all_ids))\n",
    "        #all_ids = [ele for ele in all_ids if ele != row.Previous_ID]\n",
    "        all_related_ids.append(sorted(all_ids))\n",
    "        \n",
    "    merged_df['All_Related_Ids'] = all_related_ids\n",
    "    \n",
    "    return(merged_df)\n",
    "\n",
    "def get_match_strings(related_id_cols, match_df):\n",
    "    '''\n",
    "    Iterate over each list of related ids for every row and check if id is in any of the PII_Related_ID cols.\n",
    "    If it is then append the name of the PII_Related_ID column to a string and build out the string specifying all \n",
    "    PII matched for each id in the list.\n",
    "    '''\n",
    "    confidence_df = create_match_confidence_reference()\n",
    "    \n",
    "    #Create a label of pii matched on and calculate similarity scores.\n",
    "    all_edge_labels = []\n",
    "    all_related_ids = []\n",
    "    all_confidences = []\n",
    "    for row in match_df.itertuples():\n",
    "        ids = row.All_Related_Ids\n",
    "        edge_label = []\n",
    "        new_ids = []\n",
    "        confidences = []\n",
    "        for id in ids:\n",
    "            id_matched_on = []\n",
    "            if id in row.Name_Related_Ids:\n",
    "                id_matched_on.append('Full Name')\n",
    "            if id in row.Address_Related_Ids:\n",
    "                id_matched_on.append('Full Address')\n",
    "            if id in row.Email_Related_Ids:\n",
    "                id_matched_on.append('Email')\n",
    "            if id in row.Phone_Related_Ids:\n",
    "                id_matched_on.append('Phone')\n",
    "            else:\n",
    "                #id_matched_on.append('NO MATCH')\n",
    "                pass\n",
    "            \n",
    "            sorted_id_matched_on = sorted(list(set(id_matched_on)))\n",
    "            label = ', '.join(sorted_id_matched_on)\n",
    "            \n",
    "            #Remove low confidence matches\n",
    "            match_type = confidence_df[confidence_df['Combination'] == label]['Confidence'].values[0]\n",
    "            \n",
    "            if match_type != 'Low Confidence':\n",
    "                edge_label.append(label)\n",
    "                new_ids.append(id)\n",
    "                confidences.append(match_type)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        all_related_ids.append(new_ids)\n",
    "        all_edge_labels.append(edge_label)\n",
    "        all_confidences.append(confidences)\n",
    "    match_df['Edge_Label'] = all_edge_labels\n",
    "    match_df['New_All_Related_Ids'] = all_related_ids\n",
    "    match_df['Confidence_Lists'] = all_confidences\n",
    "    return(match_df)\n",
    "\n",
    "def assign_group_id(all_related_ids):\n",
    "    '''\n",
    "    Identify a unique list of groups and assign the unique group an ID. Here there will be overlap across groupings.\n",
    "    This is the case where an id matched on email against one case but matched all other PII with other cases. \n",
    "    This would be helpful in trying to determine if fraudulent behavior is occurring across other applications.\n",
    "    '''\n",
    "    unique_lists = []\n",
    "    id = 0\n",
    "    for lst in all_related_ids:\n",
    "        if lst not in unique_lists:\n",
    "            unique_lists.append([id, sorted(lst)])\n",
    "            id += 1\n",
    "        else:\n",
    "            pass\n",
    "    return(unique_lists)\n",
    "\n",
    "def similarity_score(label):\n",
    "    '''Get the proportion of PII matched out of the total number of all PII that could be matched'''\n",
    "    score = 0\n",
    "    if 'Full Name' in label:\n",
    "        score += 1\n",
    "    if 'Full Address' in label:\n",
    "        score += 1\n",
    "    if 'Email' in label:\n",
    "        score +=1\n",
    "    if 'Phone' in label:\n",
    "        score += 1\n",
    "    return(score/4)\n",
    "\n",
    "def weighted_similarity_score(label):\n",
    "    '''Weight PII differently and get the proportion of PII matched out of the total weight of all PII'''\n",
    "    score = 0\n",
    "    if 'Full Name' in label:\n",
    "        score += 2\n",
    "    if 'Full Address' in label:\n",
    "        score += 3\n",
    "    if 'Email' in label:\n",
    "        score += 2\n",
    "    if 'Phone' in label:\n",
    "        score += 1\n",
    "    return(score/8)\n",
    "\n",
    "\n",
    "create_match_confidence_reference()\n",
    "    \n",
    "def main():\n",
    "    #Simulate fake pii data and begin iterating over each row to get list of related ids\n",
    "    df = generate_fake_pii_df(50)\n",
    "    \n",
    "    #Determine a list of ids for which to begin matching against (in this case all the ids in the data)\n",
    "    ids = df['Previous_ID'].astype(str).tolist()\n",
    "\n",
    "    columns_to_match = ['Name', 'Email', 'Phone', 'Address']\n",
    "\n",
    "    dataframes = []\n",
    "    for col in columns_to_match:\n",
    "        match_df = get_matches(df, ids, col)\n",
    "        dataframes.append(match_df)\n",
    "\n",
    "    merged_df = merge_dataframe_list(dataframes, merge_on = 'Previous_ID')\n",
    "    \n",
    "    #Create a column that specifies what each related id matched on\n",
    "    related_cols = ['Name_Related_Ids', 'Email_Related_Ids', 'Phone_Related_Ids', 'Address_Related_Ids']\n",
    "    final_df = get_match_strings(related_cols, merged_df)\n",
    "    \n",
    "    #Get a list of unique groups and assign the group an id (may have overlap between groups depending on pii matched)\n",
    "    unique_lists = assign_group_id(final_df['New_All_Related_Ids'])\n",
    "    \n",
    "    group_df = pd.DataFrame({'Group_ID' : [i[0] for i in unique_lists],\n",
    "                             'All_Related_Ids_String': [i[1] for i in unique_lists]})\n",
    "    \n",
    "    #Join the related ids together as a string so we can map the group id back to the related ids column\n",
    "    group_df['All_Related_Ids_String'] = group_df['All_Related_Ids_String'].apply(lambda x: ', '.join(x))\n",
    "    final_df['All_Related_Ids_String'] = final_df['New_All_Related_Ids'].apply(lambda x: ', '.join(x))\n",
    "    merged_df = pd.merge(final_df, group_df, on = 'All_Related_Ids_String', how = 'left')\n",
    "    merged_df = merged_df.drop(columns = 'All_Related_Ids_String')\n",
    "    \n",
    "    #Expand the 1:many dataframe to the 1:1 dataframe so that each relationship gets its own row\n",
    "    exploded_df = merged_df.explode(['New_All_Related_Ids', 'Edge_Label', 'Confidence_Lists'])\n",
    "    \n",
    "#     #Calculate similarity scores to determine how similar ids are to each other\n",
    "#     exploded_df['Similarity_Score'] = exploded_df['Edge_Label'].apply(lambda x: similarity_score(x))\n",
    "#     exploded_df['Weighted_Similarity_Score'] = exploded_df['Edge_Label'].apply(lambda x: weighted_similarity_score(x))\n",
    "    \n",
    "#     #Using a particular set of logical conditions, label the ids as a Exact Match, High, Medium, or Low Confidence duplicate \n",
    "#     func = np.vectorize(conditions)\n",
    "#     exploded_df['Is_Duplicate'] = func(exploded_df['Edge_Label'])\n",
    "    \n",
    "    return(merged_df, exploded_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0d3a0f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df, exploded_df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0bfcd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(df):\n",
    "    nodes = df['Previous_ID'].unique().tolist()\n",
    "    nodes = ['ID: ' + node for node in nodes ]\n",
    "    return(nodes)\n",
    "\n",
    "def create_edges(df):\n",
    "    edges_df = df[['Previous_ID', 'New_All_Related_Ids']]\n",
    "    edges_df = edges_df[edges_df['Previous_ID'] != edges_df['New_All_Related_Ids']]\n",
    "    edges = [('ID: ' + row['Previous_ID'], 'ID: ' + row['New_All_Related_Ids']) for index, row in edges_df.iterrows()]\n",
    "    return(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "81f822fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "edges.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"edges.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd5bb4d5d30>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True)\n",
    "\n",
    "net.add_nodes(create_nodes(exploded_df))\n",
    "net.add_edges(create_edges(exploded_df))\n",
    "net.show('edges.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26018033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c056c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# import folium\n",
    "\n",
    "# # Replace 'your_shapefile.shp' with the path to your shapefile\n",
    "# gdf = gpd.read_file('./data/Wildfires_1878_2019_Polygon_Data/Shapefile/US_Wildfires_1878_2019.shp')\n",
    "# gdf = gdf.to_crs({'proj':'longlat', 'ellps':'WGS84', 'datum':'WGS84'})\n",
    "# gdf['centroid'] = gpd.GeoSeries(gdf[\"geometry\"]).centroid\n",
    "# gdf.to_pickle('geo_dataframe_forest_fires.pkl')\n",
    "\n",
    "# my_map = folium.Map(tiles = 'cartodbdark_matter')\n",
    "# for _, r in test_df.iterrows():\n",
    "#     sim_geo = gpd.GeoSeries(r[\"geometry\"]).simplify(tolerance=0.001)\n",
    "#     geo_j = sim_geo.to_json()\n",
    "#     geo_j = folium.GeoJson(data = geo_j, style_function=lambda x: {'fillColor': 'orange',\n",
    "#                                                                    'color': 'yellow',\n",
    "#                                                                    'weight': 1,\n",
    "#                                                                    'fillOpacity': 1})\n",
    "#     #folium.Popup(r[\"FireCause\"]).add_to(geo_j)\n",
    "#     geo_j.add_to(my_map)\n",
    "# #my_map.save(\"forest_fires.html\")\n",
    "# my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65382e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest_fires_new_df = pd.read_pickle('/Users/andrewcasanova/Documents/geo_dataframe_forest_fires.pkl')\n",
    "# forest_fires_new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IFrame(src = \"forest_fires.html\", width = 800, height = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pdf.sciencedirectassets.com/271100/1-s2.0-S0379711219X00028/1-s2.0-S0379711218303941/am.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFkaCXVzLWVhc3QtMSJGMEQCIB6%2BrP729s6pOcd3RO8MOnfurh3JEkz3x5YghLvpPrNRAiBaWbgpz6TYtOMzEZKdaczL56O4QENCk5HZO1k2s7LOEyq8BQix%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMt816wIlFIr8aw6Y%2FKpAFha3vg1FILlX33M9Yn%2F%2F8Ln4q2wttgJL8A0mUR52lEptZVBRBtDKGZ6pJMEC0NymWt7hlmD4PkxD9uJszMq%2FolInsEQkMDhZcx80cUagpZYH0UR33nDrfmAyjASm%2FR28LUmIdQfkrp0JEIAVdSMxHg7PLzT4fXzYBDekJVyW1teoA4OaBoruJb%2BkGrN6iqtItrvIqbxN86HjuYbsLiKcZTQMLwUdXMK9PzsSuK7ckpuycoMu44vSPDvoNU%2BFGrPIfUAnVNqpmSBUbuOHvl5ULkiuKw8C9LBIitO%2BZcTCizvmAiM%2B6lsWKpLlBx%2BDvNrqu5Tj7mmN75E5o238XwigPV%2BDlC2Sv%2FGZgaVeDsqqawshXT2kE%2B1zrzmtVgA9J495A6Yu%2FMQZzQYgEwHB63bFMNnVonz1YOCUjKXDCXPsSfEY35KRhb04AMWoFjli3wsGlpdwTxqSqEyKhPRsS%2BVx5e%2FXvcszOgQeW4g%2FWRi2p5sY3C7LFTLFXZ%2BS5g8u0%2F4VpGJKkd6NMiAVIBKC13HXwWc%2F0hR%2BZCYCpRIqxCaLuVrcHeRIx90Zv4sdM5G4J%2FzQgnvMqOTmKcORn0v7%2BCknqN7re3cTswxh3%2FkA07LvG%2BYnkBX2mo7bqSe2IFwohM9cRdAvytsUR5Qo7IwQInCMlCN5AseMNbEhZ4os0zMHDaCHUvhaos%2FjDtG4ll0ewMgT7GQKZ5xMIetUSxB9G2C8DkaV4rvolaCebu1QLGsiaf9Sn0BT8OHqixlWpRt3qBSb%2FXengCYwKLATRoefvUfRrM9Ddfr5FRUExuca3yQVRtKz6ibnNZbUbe3bufyV%2FvUDI4ZvaxuWh13zlt1b5LL7bMYqLyyzi4ed2MdJM0emhanQwkd6UqwY6sgGboIm0vi1pPjXxvMG4nfUJoanyhSJ4KZqMOPqmaDr2TOerDPkMmnvVxUpZ1zEPPpg3ew3BhqxDz4%2Be6CDg%2FOLIUOk5njvwEpx0wAM0V0XgdGKxa57JQuk85EYUp9tLZ4%2FmfkYo84onIedu4ig%2BKOZasOPjH%2B54VFx6ekJOw1GHEDh20ZG3XJQ0ZEjTn7ilqhjvpCL1KusuN6y%2B81Pxwjj7WL3GG6PYDYm%2FLdnpwoFcRlMw&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231128T003309Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYWWV37SQY%2F20231128%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=c33d3df67240330cb588a4eb76b655ecd01191cdb98cd24c5f49c7a922309983&hash=bf059bd61fc8778f809a598de2aada60e4a3e75e672853757544641ac0ed95b2&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0379711218303941&tid=pdf-9dc187da-f80b-4720-8f62-1882dd8caf87&sid=5afc32371e344648f128f916f88a52edc0d7gxrqa&type=client\n",
    "#https://towardsdatascience.com/creating-an-interactive-map-of-wildfire-data-using-folium-in-python-7d6373b6334a\n",
    "#https://www.sciencebase.gov/catalog/item/5ee13de982ce3bd58d7be7e7\n",
    "#https://palkovic.org/wp-content/uploads/2020/12/USGS_wildfires.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139a86ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
